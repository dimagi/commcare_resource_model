estimation_buffer: 0.5
estimation_growth_factor: 0.01  # buffer increases by 1% per month
storage_buffer: 0.33  # keep max storage at 75% of disk
storage_display_unit: TB

summary_dates:
  - '2020-12'

vm_os_storage_gb: 50
vm_os_storage_group: 'VM_os'

usage:
  users:
    model: 'date_range_value'
    ranges:
      - ['20180101', '20201201', 1132]

  # LS User as a percent of total User
  ls_users:
    model: 'derived_factor'
    dependant_field: 'users'
    factor: 0.05
  # Number of formaplayer user from X date to Y
  formplayer_users:
    model: 'date_range_value'
    ranges:
      - ['20180901', '20201201', 1000]

  #Number of forms per user per month
  forms_monthly:
    model: 'derived_factor'
    dependant_field: 'users'
    factor: 0  # https://docs.google.com/spreadsheets/d/1cvgcihfE0oOrkAPW6RlmSLzGcsaD-3nDAOKVCYjyAeE/edit#gid=1011017089
  forms_daily:
    model: 'derived_factor'
    dependant_field: 'forms_monthly'
    factor: 0
  # Total Forms ever created
  forms_total:
    model: 'cumulative'
    dependant_field: 'forms_monthly'
    start_with: 16000000   # from ES form index (total docs)

  #Number of cases per user per month
  cases_total:
    model: 'baseline_with_growth'
    dependant_field: 'users'
    baseline: 0  # inflated number by MWCD. Actual probably closer to 1000
    monthly_growth: 0  # guess (4 cases for every new person case)
    start_with: 19000000   # from ES case index (total docs)

services:
  pg_shards:
    # this should have standbys for HA
    # need higher performing SSDs
    usage_capacity_per_node: 25000
    include_ha_resources: True
    storage:
      group: 'SSD'
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 1200
        - referenced_field: 'cases_total'
          unit_size: 1800
    process:
      cores_per_node: 32
      ram_per_node: 128

  pg_proxy:
    usage_capacity_per_node: 150000
    storage_scales_with_nodes: True
    storage:
      group: 'VM_other'
      static_baseline: 200GB
      override_storage_buffer: 0
      override_estimation_buffer: 0
    process:
      cores_per_node: 16
      ram_per_node: 64

  pg_main:
    usage_capacity_per_node: 150000
    storage_scales_with_nodes: True
    min_nodes: 2
    storage:
      group: 'SSD'
      static_baseline: 250GB  # to account for other static tables
      data_models:
        - referenced_field: 'users'
          unit_size: 1600
    process:
      cores_per_node: 32
      ram_per_node: 128

  couchdb:
    usage_capacity_per_node: 50000
    min_nodes: 3
    storage:
      group: 'SSD'
      redundancy_factor: 3
      static_baseline: 50GB  # to account for other databases
      override_storage_buffer: 0.8  # space for compaction
      data_models:
        - referenced_field: 'users'
          unit_size: 600000   # disk size / doc count of icds @ 2017-12-13
    process:
      cores_per_node: 16
      ram_per_node: 64

  es_datanode:
    usage_capacity_per_node: 15000
    min_nodes: 3
    storage:
      group: 'SAS'
      redundancy_factor: 2
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 5500
        - referenced_field: 'cases_total'
          unit_size: 1800
    process:
      cores_per_node: 16
      ram_per_node: 64

  es_master:
    static_number: 6
    storage_scales_with_nodes: True
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0
    process:
      cores_per_node: 8
      ram_per_node: 16

  riakcs:
    usage_capacity_per_node: 50000
    # avg attachment size of 12560 bytes (11000*0.96 + 50000*0.04)
    # RAM requirement per key = 130b
    # num keys = 10TB / (12560b x 3<redundancy factor>)
    # RAM needed = 130b x num keys = 35GB (64GB avail per node)
    max_storage_per_node: 25TB
    min_nodes: 10
    storage:
      group: 'SSD'
      redundancy_factor: 3
      static_baseline: 10TB  # to account for exports etc
      data_models:
        - referenced_field: 'forms_total'  # 96% of objects
          unit_size: 11000
    process:
      # current load quite low (~15%) (2018-10-11)
      cores_per_node: 32
      # need to be able to fit all keys in RAM since we're using bitcask backend
      # Current usage is at 30% (2018-10-11)
      ram_per_node: 128
      ram_model:
      - referenced_field: 'forms_total'
        # key size (45 + 6 + 79) (overhead + bucket + key)
        # bucket = 'blobdb'
        # new keys are smaller but stick with old key length for safety:
        #   new: form/xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx/Xpi-XM9CZvQ
        #   old: form/xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx/form.xml.xxxxxxxxxxxxxxuuidxxxxxxxxxxxxxx
        unit_size: 130
      ram_redundancy_factor: 3
      ram_static_baseline: 1  # per node

  pg_ucr:
    usage_capacity_per_node: 100000
    storage_scales_with_nodes: True
    min_nodes: 2
    storage:
      # This is a rough estimate.
      # The person case UCR is 35% of total UCR usage.
      group: 'SSD'
      data_models:
        - referenced_field: 'cases_total'  # cumulative
          unit_size: 4000  # to account for monthly data etc.
    process:
      cores_per_node: 32
      ram_per_node: 256

  pg_warehouse:
    usage_capacity_per_node: 300000
    storage_scales_with_nodes: True
    storage:
      group: 'SSD'
      data_models:
        - referenced_field: 'forms_total'
          unit_size: 4000  # to account for monthly data etc.
    process:
      cores_per_node: 16
      ram_per_node: 128

  pillowtop:  # will need to revamp this once combined pillows are rolled out
    storage_scales_with_nodes: True
    process:
      cores_per_node: 16
      ram_per_node: 48
      cores_per_sub_process: 0.5
      ram_per_sub_process: 0.7
      sub_processes:
        - name: 'other'
          capacity: 7000  # covers all other pillows
        - name: 'FormSubmissionMetadataTrackerPillow'
          capacity: 7000
        - name: 'XFormToElasticsearchPillow'
          capacity: 7000
        - name: 'kafka-ucr-static-forms'
          capacity: 2500
        - name: 'kafka-ucr-static-cases'
          capacity: 1000
        - name: 'CaseToElasticsearchPillow'
          capacity: 1000
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  celery:
    storage_scales_with_nodes: True
    process:
      cores_per_node: 16
      ram_per_node: 48
      cores_per_sub_process: 0.7  # reduced from 1 since we use django VMs as well
      ram_per_sub_process: 0.5  # reduced from 0.7 since we use django VMs as well
      sub_processes:
        - name: 'reminder_case_update_queue'
          capacity: 1000
        - name: 'reminder_queue'
          capacity: 30000
        - name: 'ucr_indicator_queue'
          capacity: 5000
        - name: 'icds_dashboard_reports_queue'
          capacity: 7000
        - name: 'sms_queue'
          capacity: 15000
        - name: 'case_rule_queue'
          capacity: 60000
        - name: 'reminder_rule_queue'
          capacity: 60000
        - name: 'submission_reprocessing_queue'
          capacity: 60000
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  django:
    usage_capacity_per_node: 15000
    storage_scales_with_nodes: True
    process:
      cores_per_node: 16
      ram_per_node: 48
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  redis:
    usage_capacity_per_node: 200000
    min_nodes: 3
    process:
      cores_per_node: 12
      ram_per_node: 96
      ram_model:
        - referenced_field: 'users'
          unit_size: 50KB
      ram_static_baseline: 33  # per node (assume only 50% ram is usable)
    storage:
      group: 'SAS'
      data_models:
        - referenced_field: 'users'
          unit_size: 50KB

  nginx:  # limits for nginx not clear
    usage_capacity_per_node: 250000
    storage_scales_with_nodes: True
    process:
      cores_per_node: 16
      ram_per_node: 64
    storage:
      group: 'VM_other'
      static_baseline: 250GB  # logs etc
      override_storage_buffer: 0
      override_estimation_buffer: 0

  rabbitmq:  # limits for rabbitmq not clear
    static_number: 4
    storage_scales_with_nodes: True
    process:
      cores_per_node: 32
      ram_per_node: 128
    storage:  # don't have a model for rabbitmq storage
      group: 'SAS'
      static_baseline: 500GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  airflow:
    static_number: 4
    storage_scales_with_nodes: True
    process:
      cores_per_node: 8
      ram_per_node: 32
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0

  control:
    static_number: 1
    storage:
      group: 'VM_other'
      static_baseline: 100GB
      override_storage_buffer: 0
      override_estimation_buffer: 0
    process:
      cores_per_node: 4
      ram_per_node: 8
